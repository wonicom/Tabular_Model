# -*- coding: utf-8 -*-
"""ensemble.ipynb_setting02의 softvoting

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jxc4_vJUvT7KbXawl6EsozN1GE6CmHT7
"""

!pip install pytorch-tabnet

import numpy as np
import pandas as pd
import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from pytorch_tabnet.tab_model import TabNetClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.svm import SVC

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/gas_sensor_drift_final/Dataset'
files = os.listdir(path)

dataframes = []

for file in files :
  if file.endswith('.dat') :
    df = pd.read_csv(os.path.join(path, file), sep='\s+', header=None)
    dataframes.append(df)

def extract_value(x):
      parts = str(x).split(':')
      if len(parts) == 2:
          return float(parts[1])

for files in dataframes:
    for col in files.columns[1:]:
        files[col] = files[col].apply(lambda x: extract_value(x))

for i in range(0,10) :
  dataframes[i].rename(columns={0:'Target'},inplace = True)

for j in range(0,10) :

  map={1:0,2:1,3:2,4:3,5:4,6:5}
  dataframes[j]["Target"] = dataframes[j]["Target"].map(map)

# Soft Voting 앙상블을 위한 분류기 정의(chatgpt출처)
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.metrics import accuracy_score


class SoftVotingClassifier:
    def __init__(self, models):
        self.models = models

    def predict(self, X):
        predictions = np.zeros((X.shape[0], len(self.models)))
        for i, model in enumerate(self.models):
            if hasattr(model, 'predict_proba'):
                predictions[:, i] = model.predict_proba(X)[:, 1]
            else:
                predictions[:, i] = model.predict(X)

        avg_predictions = np.mean(predictions, axis=1)
        ensemble_predictions = np.where(avg_predictions > 0.5, 1, 0)
        return ensemble_predictions

for f in range(1,10) :
  f_temp = []
  temp_df = pd.concat([dataframes[i] for i in range(f)], axis=0)
  print(temp_df.shape)

import tensorflow as tf

def pearson_correlation_loss(y_true, y_pred):
    # 피어슨 상관계수 계산
    mean_true = tf.reduce_mean(y_true)
    mean_pred = tf.reduce_mean(y_pred)
    num = tf.reduce_sum((y_true - mean_true) * (y_pred - mean_pred))
    den = tf.sqrt(tf.reduce_sum((y_true - mean_true)**2) * tf.reduce_sum((y_pred - mean_pred)**2))
    corr = num / den

    # 최소화할 음의 상관계수
    return -corr

import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical
from keras.layers import LSTM, BatchNormalization

epochs = 20

for f in range(1,10) :
  f_temp = []
  temp_df = pd.concat([dataframes[i] for i in range(0, f)], axis=0)


  X_test = dataframes[f].iloc[:, 1:129]
  X_train = temp_df.iloc[:, 1:129]
  y_test = dataframes[f].iloc[:, 0]
  y_train = temp_df.iloc[:, 0]

  #결측치를 중앙값으로 대체
  X_train = X_train.fillna(X_train.median())
  X_test = X_test.fillna(X_test.median())
  y_train = y_train.fillna(y_train.median())
  y_test = y_test.fillna(y_test.median())

  y_train = y_train.values.ravel()
  y_test = y_test.values.ravel()

  from sklearn.preprocessing import StandardScaler
  sc_X = StandardScaler()
  X_train_scaled = sc_X.fit_transform(X_train)
  X_test_scaled = sc_X.transform(X_test)

  # One-hot encode your labels (if not done elsewhere)
  # y_train_onehot = to_categorical(y_train)
  # y_test_onehot = to_categorical(y_test)

   # TabNet 모델 구축

  tabnet_model = TabNetClassifier(
        seed =0,
        verbose=1,
        )

  tabnet_model.fit(
    X_train=X_train_scaled, y_train=y_train,
    max_epochs=20,
    patience=16,
    batch_size=16,
    virtual_batch_size=128,
    num_workers=0,
    drop_last=True
    )

  tabnet_preds = tabnet_model.predict_proba(X_test_scaled)

  # SVM 모델 구축

  svm_model = SVC(kernel='linear', C=1.0, probability=True)
  svm_model.fit(X_train_scaled,y_train)
  svm_preds = svm_model.predict_proba(X_test_scaled)

  X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0],1,X_train_scaled.shape[1])
  X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0],1,X_test_scaled.shape[1])


  #LSTM 모델 구축

  from keras.models import Sequential  # 필요한 패키지 추가
  from keras.layers import Dense, Dropout  # 필요한 패키지 추가

  lstm_model = Sequential()
  lstm_model.add(LSTM(256, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]), activation='relu', return_sequences=True))
  lstm_model.add(Dropout(0.2))
  lstm_model.add(LSTM(128, activation='relu'))

  num_classes = len(np.unique(y_train))
  lstm_model.add(Dense(num_classes, activation='softmax'))

  from keras.optimizers import Adam
  lstm_model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
  lstm_model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=16)


  batch_size = 32  # 필요에 따라 조정
  lstm_model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=batch_size)

  pred_lstm = lstm_model.predict(X_test_scaled)

  ensemble_pred = (pred_lstm + tabnet_preds +svm_preds) / 3
  ensemble_preds = np.argmax(ensemble_pred, axis=1)

  #앙상블 분류기 평가
  tabnet_preds = np.argmax(tabnet_preds, axis=1)
  tabnet_accuracy = accuracy_score(y_test, tabnet_preds)
  print("Tabnet Accuracy:", tabnet_accuracy)

  svm_preds = np.argmax(svm_preds, axis=1)
  svm_accuracy = accuracy_score(y_test, svm_preds)
  print("SVM Accuracy:", svm_accuracy)

  lstm_preds = np.argmax(pred_lstm, axis=1)
  lstm_accuracy = accuracy_score(y_test, lstm_preds)
  print("LSTM Accuracy:", lstm_accuracy)

  ensemble_accuracy = accuracy_score(y_test, ensemble_preds)
  print("Ensemble Accuracy:", ensemble_accuracy)

pred_resized = pred[:445]

print(pred_expanded.shape)
print(X_train_scaled.shape)

X_train_combined = np.concatenate((X_train_scaled, pred), axis=1)
X_train_combined

from sklearn.svm import SVC

# 결합된 특성으로 SVM 학습
svm_model = SVC(kernel='linear')
svm_model.fit(X_train_combined, y_train)

"""Model02"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_test_scaled = scaler.transform(X_test)
lstm_probabilities_test = lstm_model.predict(X_test_scaled)
X_test_combined = np.concatenate((X_test_scaled, lstm_probabilities_test), axis=1)

# SVM을 사용하여 예측 수행
y_pred = svm_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"앙상블 모델 정확도: {accuracy}")

"""앙상블 모델 구축"""

from sklearn.ensemble import VotingClassifier

ensemble_model = VotingClassifier(estimators=[('lstm', lstm_model), ('svm', classifier)], voting='soft')

ensemble_model.fit(X_train, y_train)

# 앙상블 모델 평가
y_pred = ensemble_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Ensemble Model Accuracy: {accuracy}")