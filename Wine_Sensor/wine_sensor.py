# -*- coding: utf-8 -*-
"""gas_sensor

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ExwfSJOf7m4Eq3tjr71Ig_nr5pxS6znM
"""

import torch
import torch.nn as nn
import os
import pandas as pd
from scipy.stats import kurtosis, skew, entropy
import numpy as np

import torchvision.transforms as transforms

import torch.nn.functional as F
import time

from torch.utils.data import Dataset, TensorDataset # 텐서데이터셋
from torch.utils.data import TensorDataset # 텐서데이터셋
from torch.utils.data import DataLoader # 데이터로더

path = '/content/drive/MyDrive/final/sample/test/blackcurrant_sample.csv'
df = pd.read_csv(path)
df.head()

os.environ['HOME']

ROOT = '/content/drive/MyDrive/final'

data_dir = os.path.join(ROOT, 'sample')
train_dir = os.path.join(data_dir,'train')
val_dir = os.path.join(data_dir, 'val')
test_dir = os.path.join(data_dir,'test')

os.getcwd()

print(train_dir)

train_csv_list = os.listdir(train_dir)
val_csv_list = os.listdir(val_dir)
test_csv_list = os.listdir(test_dir)

print(val_csv_list)

train_path_list = []
val_path_list = []
test_path_list = []

for csv in train_csv_list :
  if '.csv' in csv :
    train_path = os.path.join(data_dir,'train',csv)
    train_path_list.append(train_path)

for csv in val_csv_list :
  if '.csv' in csv :
    val_path = os.path.join(data_dir,'val',csv)
    val_path_list.append(val_path)

for csv in test_csv_list :
  if '.csv' in csv :
    test_path = os.path.join(data_dir,'test',csv)
    test_path_list.append(test_path)

print(train_path_list)
print(val_path_list)
print(test_path_list)

train_df_list = []
val_df_list = []
test_df_list = []

for path in train_path_list :
  df = pd.read_csv(path)
  df = df[['CO','VOC','C2H5OH','NO2']]
  df = df.dropna(axis=0)
  train_df_list.append(df)

for path in val_path_list :
  df = pd.read_csv(path)
  df = df[['CO','VOC','C2H5OH','NO2']]
  df = df.dropna(axis=0)
  val_df_list.append(df)

for path in test_path_list :
  df = pd.read_csv(path)
  df = df[['CO','VOC','C2H5OH','NO2']]
  df = df.dropna(axis=0)
  test_df_list.append(df)

print(len(train_df_list))
print(len(val_df_list))
print(len(test_df_list))

print(test_df_list)

train_tensor = []
val_tensor = []
test_tensor = []

for df in train_df_list :
  idx = df[df['VOC'] == "DATA"].index
  df.drop(idx , inplace=True)
  df = df.astype(float)
  tensor = torch.tensor(df.values)
  train_tensor.append(tensor)

for df in val_df_list :
  idx = df[df['VOC'] == "DATA"].index
  df.drop(idx , inplace=True)
  df = df.astype(float)
  tensor = torch.tensor(df.values)
  val_tensor.append(tensor)

for df in test_df_list :
  idx = df[df['VOC'] == "DATA"].index
  df.drop(idx , inplace=True)
  df = df.astype(float)
  tensor = torch.tensor(df.values)
  test_tensor.append(tensor)

tensor.size()#12,5,4

print(test_tensor[0].shape)

train_samples1 = torch.split(train_tensor[0], 5) #한번에 쓰기에는 너무 적어서 / 5초의 향만 가지고 있으면 돼! / 여러개로 나누어서 쓰자!/샘플링(sampling)
train_samples2 = torch.split(train_tensor[1], 5)
train_samples3 = torch.split(train_tensor[2], 5)
train_samples4 = torch.split(train_tensor[3], 5)
train_samples5 = torch.split(train_tensor[4], 5)

val_samples1 = torch.split(val_tensor[0], 5)
val_samples2 = torch.split(val_tensor[1], 5)
val_samples3 = torch.split(val_tensor[2], 5)
val_samples4 = torch.split(val_tensor[3], 5)
val_samples5 = torch.split(val_tensor[4], 5)

test_samples1 = torch.split(test_tensor[0], 5)
test_samples2 = torch.split(test_tensor[1], 5)
test_samples3 = torch.split(test_tensor[2], 5)
test_samples4 = torch.split(test_tensor[3], 5)
test_samples5 = torch.split(test_tensor[4], 5)

print(len(train_samples1)) #샘플의 수 샘플 하나의 shape 5,4
print(type(train_samples1))
print(train_samples1[0])

print(len(val_samples1)) #샘플의 수 샘플 하나의 shape 5,4
print(type(val_samples1))
print(val_samples1[0])

print(len(test_samples1)) #샘플의 수 샘플 하나의 shape 5,4
print(type(test_samples1))
print(test_samples1[0])

train_input1 = torch.stack(train_samples1[:-1],dim=0)
train_input2 = torch.stack(train_samples2[:-1],dim=0)
train_input3 = torch.stack(train_samples1[:-1],dim=0)
train_input4 = torch.stack(train_samples2[:-1],dim=0)
train_input5 = torch.stack(train_samples2[:-1],dim=0)

val_input1 = torch.stack(val_samples1[:-1],dim=0)
val_input2 = torch.stack(val_samples2[:-1],dim=0)
val_input3 = torch.stack(val_samples1[:-1],dim=0)
val_input4 = torch.stack(val_samples2[:-1],dim=0)
val_input5 = torch.stack(val_samples2[:-1],dim=0)

test_input1 = torch.stack(test_samples1[:-1],dim=0)
test_input2 = torch.stack(test_samples2[:-1],dim=0)
test_input3 = torch.stack(test_samples1[:-1],dim=0)
test_input4 = torch.stack(test_samples2[:-1],dim=0)
test_input5 = torch.stack(test_samples2[:-1],dim=0)

print(train_input1.shape)
print(train_input2.shape)

print(val_input1.shape)
print(val_input2.shape)

print(test_input1.shape)
print(test_input2.shape)

train_y1 = []
for i in range(len(train_input1)) :
  train_y1.append(0)

train_y2 = []
for j in range(len(train_input2)) :
  train_y2.append(1)

train_y3 = []
for i in range(len(train_input3)) :
  train_y3.append(2)

train_y4 = []
for j in range(len(train_input4)) :
  train_y4.append(3)

train_y5 = []
for i in range(len(train_input5)) :
  train_y1.append(4)




val_y1 = []
for i in range(len(val_input1)) :
  val_y1.append(0)

val_y2 = []
for j in range(len(val_input2)) :
  val_y2.append(1)

val_y3 = []
for i in range(len(val_input3)) :
  val_y3.append(2)

val_y4 = []
for j in range(len(val_input4)) :
  val_y4.append(3)

val_y1 = []
for i in range(len(val_input5)) :
  val_y1.append(4)


test_y1 = []
for i in range(len(test_input1)) :
  test_y1.append(0)

test_y2 = []
for j in range(len(test_input2)) :
  test_y2.append(1)

test_y3 = []
for i in range(len(test_input3)) :
  test_y1.append(2)

test_y4 = []
for j in range(len(test_input4)) :
  test_y2.append(3)

test_y5 = []
for i in range(len(test_input5)) :
  test_y1.append(4)

print(len(train_y1))
print(len(train_y2))

print(len(val_y1))
print(len(val_y2))

print(len(test_y1))
print(len(test_y2))

train_x = torch.cat([train_input1,train_input2,train_input3,train_input4,train_input5],dim=0)
val_x = torch.cat([val_input1,val_input2,val_input3,val_input4,val_input5],dim=0)
test_x = torch.cat([test_input1,test_input2,test_input3,test_input4,test_input5],dim=0)

print(train_x.shape)
print(val_x.shape)
print(test_x.shape)

train_y = train_y1+train_y2
val_y = val_y1+val_y2
test_y = test_y1+test_y2

print(len(train_y))
print(len(val_y))

print(len(test_y))

class CustomDataset(Dataset): #파이토치가 이해할 수 있는 식으로 만들기 위해서 (파이토치형 데이터셋) #파이토치가 연산할 수 있도록록
    def __init__(self, x, y,):
        self.x = x
        self.y = y

    def __getitem__(self, index):
        x = self.x[index]
        y = self.y[index]
        return x, y

    def __len__(self):
        return len(self.x)

train_dataset = CustomDataset(train_x,train_y)
val_dataset = CustomDataset(val_x,val_y)
test_dataset = CustomDataset(test_x,test_y)

print(train_dataset[0])
print(val_dataset[0])
print(test_dataset[0])

train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)

class LSTM(nn.Module):
    def __init__(self, n_layers, hidden_dim, n_classes, dropout_p = 0.2):
        super(LSTM, self).__init__()
        self.dropout = nn.Dropout(dropout_p) #일부 파라미터를 학습에 반영하지 않음으로써 모델을 일반화하는 방법
         #주의 : Train시에는 Dropout을 적용해야 하지만 Validation, Test에는 적용하면 안됨
        self.lstm = nn.LSTM(4, hidden_dim, num_layers=n_layers, batch_first= True,  bidirectional=True)
        self.out = nn.Linear(hidden_dim*2, n_classes, bias=True)

    def forward(self, x):
        outputs,(hidden, cell) = self.lstm(x)
        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) #hidden state에 대한 처리
        # hidden의 마지막 두 개의 타임 스텝의 hidden state를 가져와서, torch.cat 함수를 사용하여 이 두 개의 hidden state를 가로로(concatenate) 이어 붙인 후, self.dropout 함수를 통해 dropout을 적용하는 것
        logit = self.out(hidden)
        return logit

USE_CUDA = torch.cuda.is_available()
DEVICE = torch.device("cuda" if USE_CUDA else "cpu")
print("cpu 와 cuda 중 다음 기기로 학슴함: ", DEVICE)

n_classes = 5
model = LSTM(3, 256,  n_classes).to(DEVICE) #LSTM(input_size,hidden_size,vocab_size(어휘(vocabulary)의 크기),batch_size,class의 갯수)
lr = 0.0001
optimizer = torch.optim.Adam(model.parameters(), lr = lr)

def train(model, optimizer, train_iter):
    model.train() #학습모드로 설정
    corrects, total_loss = 0, 0 #각각 정확한 예측의 개수와 총 손실(loss)을 저장하는 변수들을 초기화
    size = 0 #학습 데이터의 총 개수를 저장하는 변수로 초기화
    for b, batch in enumerate(train_iter): #데이터 로더 #train_iter를 통해 미니 배치(mini-batch) 단위로 데이터를 로드하고, 해당 데이터를 DEVICE로 이동합니다.
        x , y = batch #x - 토큰들에 대한 정보(2d) , l-정수, y-정수
        x = x.float().to(DEVICE)
        y = y.long().to(DEVICE)
        y = y.reshape(-1)
        optimizer.zero_grad() #그래디언트를 초기화
        logit = model(x) #model(x, l)은 입력 데이터 x와 시퀀스 길이(l)를 모델에 입력하여 예측값(logit)을 얻습니다.
        loss = F.cross_entropy(logit, y, reduction="sum") #reduction="sum" 인자는 손실을 미니 배치 내의 모든 예측값에 대해 합산하는것것
        total_loss += loss.item()
        loss.backward()
        optimizer.step()
        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum() #logit.max(1)[1]은 로짓(logit)의 각 행마다 최대값을 가지는 인덱스를 구하는 연산
        #logit.max(1)은 각 행에서 가장 큰 값을 가지는 열의 값을 반환한다 [1]은 최댓값 인덱스를 가져온다
        #view(y.size())는 구해진 인덱스 텐서의 크기를 y 텐서와 동일하게 변경하는 연산
        size += x.shape[0]
    avg_loss = total_loss / size
    avg_accuracy = 100.0 * corrects / size
    return avg_loss, avg_accuracy

def evaluate(model, val_iter):
    model.eval() #model.eval()은 PyTorch에서 모델을 평가 모드로 전환하는 데 사용되는 방법입니다.
    corrects, total_loss = 0, 0
    size = 0
    with torch.no_grad():
        for batch in val_iter:
            x , y = batch
            x = x.float().to(DEVICE)
            y = y.long().to(DEVICE)
            y = y.reshape(-1)
            logit = model(x)
            loss = F.cross_entropy(logit, y, reduction="sum")
            total_loss += loss.item()
            corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()
            size += x.shape[0]
    avg_loss = total_loss / size
    avg_accuracy = 100.0 * corrects / size
    return avg_loss, avg_accuracy

def epoch_time(start_time, end_time):#학습과정에 소요되어지는 시간
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

best_val_loss = None
n_epochs = 15
for epoch in range(n_epochs):

    start_time = time.time() #작업실행시간 측정

    train_loss, train_accuracy = train(model, optimizer, train_dataloader)
    val_loss, val_accuracy = evaluate(model, val_dataloader)

    end_time = time.time()

    epoch_mins, epoch_secs = epoch_time(start_time, end_time)#epoch_time()함수를 호출하여 각 에포크의 소요시간을 분과 초로 계산

    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_accuracy:.2f}%')
    print(f'\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_accuracy:.2f}%')

    if not best_val_loss or val_loss < best_val_loss:
        torch.save(model.state_dict(), "./classificatior.pt")
        best_val_loss = val_loss

model.load_state_dict(torch.load("./classificatior.pt"))

test_loss, test_accuracy = evaluate(model, test_dataloader)
print(test_accuracy)

